{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimental Part**\n",
    "\n",
    "В данной работе рассматриваются различные подходы к построению Retrieval-Augmented Generation (RAG), включая:\n",
    "\n",
    "- **Trivial RAG** — базовый подход к RAG;\n",
    "- **Trivial RAG + BM25** — улучшенный вариант с использованием алгоритма BM25 для более точного поиска релевантной информации;\n",
    "- **Graph RAG** — подход, который сочетает использование базы знаний (knowledge base) с Trivial RAG.\n",
    "\n",
    "Перед реализацией указанных подходов будет проведена аннотация отдельных чанков (фрагментов данных) с использованием вариации метода **Contextual Retrieval**, который позволит более точно извлекать и обогощать информацию из различных модулей кода проекта.\n",
    "\n",
    "Для оценки качества реализованных подходов будет использован инструмент **RAGAS**, с помощью которого будут сформированы примитивные тесты, позволяющие объективно сравнить результаты работы различных методов.\n",
    "\n",
    "Основные шаги, предусмотренные в ходе работы:\n",
    "\n",
    "1. **Аннотация данных:**  \n",
    "   - Разделение проекта на чанки;  \n",
    "   - Аннотирование чанков с применением Contextual Retrieval.\n",
    "\n",
    "2. **Реализация RAG-подходов:**  \n",
    "   - Разработка и настройка Trivial RAG, Trivial RAG + BM25, Graph RAG.\n",
    "\n",
    "3. **Оценка качества:**  \n",
    "   - Создание тестов с использованием RAGAS;  \n",
    "   - Сравнительный анализ результатов.\n",
    "\n",
    "Результаты экспериментов помогут определить наиболее эффективный подход к построению RAG для решения поставленных задач.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing and Annotation**\n",
    "\n",
    "Данный подход наиболее полно рассмотрен в отдельной тетрадке под названием **data_prprocessing**, поэтому подробно на нём останавливаться не будем, а лишь перечислим статьи, в которых описаны использованные методы:\n",
    "\n",
    "- [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://arxiv.org/abs/2411.06037)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовый запрос для проверки работоспособности\n",
    "query = \"\"\"\n",
    "вот детальная спецификация Предоставленная информация и требования позволят составить детальную спецификацию задач для реализации функциональности отображения данных из формы обратной связи. В основе решения будет лежать применение технологии Java Spring Boot для бэкенда и использование базы данных PostgreSQL.\n",
    "Спецификация задач Бэкенд-разработка API для получения списка обращений\n",
    "Ендпоинт: GET /api/v1/feedback\n",
    "Функциональность:\n",
    "Возвращает список обращений. Поддерживает фильтрацию параметрами: type и priority. Поддерживает сортировку по параметру created_at, с возможностью указания порядка (возрастание или убывание). Реализация:\n",
    "Использовать Spring Data JPA для управления запросами, включая Specification интерфейс для динамических запросов с фильтрацией и сортировкой. Конструировать SQL запросы с использованием Criteria API или JPQL. Возвращаемый ответ:\n",
    "JSON-массив объектов, каждый из которых включает id, user_id, type, priority, description, и created_at. API для получения детальной информации об обращении\n",
    "Ендпоинт: GET /api/v1/feedback/{{id}}\n",
    "Функциональность:\n",
    "Возвращает полную информацию об обращении, включая связанные файлы. Реализация:\n",
    "Использовать JPA для получения данных из таблицы feedback. Использовать join-запросы или в запросе использовать feedback_documents для сопоставления с таблицей document. Возвращаемый ответ:\n",
    "JSON-объект с подробными данными обращения и массив URL-адресов на прикрепленные файлы. Метод сохранения файла в Document:\n",
    "Метод: public Document create(MultipartFile file, UserPrincipal principal, boolean useOpenAI) Изменения: Добавить флаг useOpenAI для определения необходимости загрузки файлов в OpenAI. Логика загрузки: String fileApiId = null; if (useOpenAI) { fileApiId = openaiClient.uploadFile(file); } String uploadedObjectName = minioService.uploadFile(file);\n",
    "Document document = new Document(); document.setOriginalFileName(file.getOriginalFilename()); document.setObjectName(uploadedObjectName); document.setUser(user); document.setSize(file.getSize()); document.setFileApiId(fileApiId);\n",
    "return documentRepository.save(document); Базы данных:\n",
    "Миграции: Использовать инструмент миграции схемы базы данных, такой как Flyway или Liquibase, для создания и обновления таблиц feedback, document, и feedback_documents.\n",
    "напиши мне код на Spring Boot для реализации. Учитывай что мы сейчас используем DTO для передачи данных.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trivial RAG**\n",
    "\n",
    "Для гибкости в данном разделе не везде будем использовать встроенные методы/классы langchain, но во благо гибкости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from typing import List, Tuple\n",
    "import dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После аннотирования отдельных фрагментов кода (которые в данном случае эквивалентны чанкам) с использованием подхода **Contextual Retrieval**, мы объединили их в единый документ с обогащенным контекстом. Далее документ будет повторно разделён на чанки, которые станут основой для обучения модели с применением следующих подходов: **Trivial RAG, RAG + BM25** и **Graph RAG**.\n",
    "\n",
    "В соответствии с нашим аннотирования используем strategy chunking по символу (символьному слову).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('backend.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"Файл:\",\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Фильтруем чанки, оставляя только те, что меньше 8192 токенов (бейзлайн обусловленный длинной входного контекста модели)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "filtered_chunks = []\n",
    "for chunk in chunks:\n",
    "    num_tokens = len(encoding.encode(chunk))\n",
    "    if num_tokens <= 8192:\n",
    "        filtered_chunks.append(chunk)\n",
    "\n",
    "print(f\"Total {len(filtered_chunks)} chunks received after filtering\")\n",
    "for i, chunk in enumerate(filtered_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk.strip())\n",
    "    print(f\"Chunk size: {len(chunk)} characters\")\n",
    "    print(f\"Number of tokens: {len(encoding.encode(chunk))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также посмотрим на количество токенов в нашем документе, чтобы продемонстрировать что число токенов >100_000, что является верзней границей для чистого применения LLM и говорит об необходимости использования RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_text(text: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_text(text, \"o200k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним генерацию ембеддингов для каждого чанка, чтобы использовать в качестве векторов для RAG.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_embeddings(input_texts: List[str], model: str = \"text-embedding-ada-002\") -> List[List[float]]:\n",
    "    \"\"\"Генерация эмбеддингов с помощью OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        input_texts: список текстов (чанки) для генерации эмбеддингов\n",
    "        model: модель для генерации эмбеддингов (по умолчанию text-embedding-ada-002), взяли просто проприетарную\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: список векторов эмбеддингов для каждого входного текста\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=input_texts,\n",
    "        model=model\n",
    "    )\n",
    "    return [data.embedding for data in response.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(filtered_chunks)\n",
    "\n",
    "print(f\"A total of {len(embeddings)} embedding vectors have been generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я теперь реализуем простой vector retrieval для нашего документа, который будет использоваться для trivial RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_retrieval(query: str, top_k: int = 5) -> List[int]:\n",
    "    \"\"\"\n",
    "    Получение top-k наиболее релевантных документов на основе косинусного сходства (бейзлайн, остальные метрики сходства аргументировать сложнее) между\n",
    "    эмбеддингом запроса и эмбеддингами документов.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Текст запроса\n",
    "        top_k (int): Количество документов для получения (по умолчанию 5)\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: Список индексов top-k наиболее релевантных документов\n",
    "    \"\"\"\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    \n",
    "    # Вычисляем косинусное сходство между запросом и всеми документами\n",
    "    similarities = cosine_similarity([query_embedding], embeddings_array)[0]\n",
    "    \n",
    "    # Получаем индексы top-k документов с наибольшим сходством\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return list(top_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retrieval(query, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учетом всего вышеуказанного кода (плохая практика, что он в глобальном пространстве), мы сформируем класс **TrivialRAG** для удобства обращения и рпдеоставления единого интерфейса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_templates import simple_rag_prompt\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, embeddings: List[np.ndarray], chunks: List[str], client: OpenAI, prompt_template: str = simple_rag_prompt):\n",
    "        \"\"\"\n",
    "        Инициализация класса SimpleRAG.\n",
    "\n",
    "        Args:\n",
    "            embeddings (List[np.ndarray]): Эмбеддинги документов\n",
    "            chunks (List[str]): Исходные текстовые чанки\n",
    "            client (OpenAI): клиент OpenAI\n",
    "            prompt_template (str): Шаблон промпта (по умолчанию загружается из prompt_templates)\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = chunks\n",
    "        self.client = client\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "    def _get_relevant_chunks(self, query: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Получение релевантных чанков на основе косинусного сходства.\n",
    "\n",
    "        Args:\n",
    "            query (str): Текст запроса\n",
    "            top_k (int): Количество чанков для получения\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список релевантных чанков\n",
    "        \"\"\"\n",
    "        query_embedding = generate_embeddings([query])[0]\n",
    "        embeddings_array = np.array(self.embeddings)\n",
    "        similarities = cosine_similarity([query_embedding], embeddings_array)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [self.chunks[i] for i in top_indices]\n",
    "\n",
    "    def retrieve_and_generate(self, query: str, top_k: int = 5) -> Tuple[List[str], str]:\n",
    "        \"\"\"\n",
    "        Получение релевантных чанков и генерация ответа.\n",
    "\n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "            top_k (int): Количество релевантных чанков\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], str]: Кортеж из списка использованных чанков и сгенерированного ответа\n",
    "        \"\"\"\n",
    "        relevant_chunks = self._get_relevant_chunks(query, top_k)\n",
    "        context = \"\\n\\n\".join(relevant_chunks)\n",
    "        \n",
    "        prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return relevant_chunks, response.choices[0].message.content\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[int], str]:\n",
    "        \"\"\"\n",
    "        Основной метод для получения ответа на запрос.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "            top_k (int): Количество релевантных чанков\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[int], str]: Кортеж из индексов использованных чанков и сгенерированного ответа\n",
    "        \"\"\"\n",
    "        relevant_chunks, answer = self.retrieve_and_generate(query, top_k)\n",
    "        chunk_indices = [self.chunks.index(chunk) for chunk in relevant_chunks]\n",
    "        return answer, chunk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag = SimpleRAG(embeddings=embeddings, chunks=chunks, client=client)  \n",
    "simple_response, _ = simple_rag.retrieve(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(simple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trivial RAG + BM25**\n",
    "\n",
    "Комбинированный подход, сочетающий поиск по эмбеддингам и статистический метод **BM25**, представляет собой более расширенную практику, которая демонстрирует высокую эффективность. Данный метод был подробно описан в [статье Anthropic, опубликованной в сентябре 2024 года](https://www.anthropic.com/news/contextual-retrieval). \n",
    "\n",
    "В следующем разделе мы реализуем данный подход на практике.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = bm25s.BM25(corpus=filtered_chunks)\n",
    "retriever.index(bm25s.tokenize(filtered_chunks))\n",
    "results, scores = retriever.retrieve(bm25s.tokenize(query), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_retrieval(query: str, top_k: int = 5) -> List[int]:\n",
    "    \"\"\"\n",
    "    Получение top-k наиболее релевантных документов с помощью алгоритма BM25.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Текст запроса\n",
    "        top_k (int): Количество документов для получения (по умолчанию 5)\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: Список индексов top-k наиболее релевантных документов\n",
    "    \"\"\"\n",
    "    # Получаем результаты и оценки релевантности с помощью BM25\n",
    "    results, _ = retriever.retrieve(bm25s.tokenize(query), k=top_k)\n",
    "    \n",
    "    # Преобразуем результаты в индексы исходных документов\n",
    "    return [filtered_chunks.index(doc) for doc in results[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retrieval(query , top_k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим вычисление reciprocal rank fusion для нашего документа, который будет использоваться для полуения совместных результатов с bm25 и embedding retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(*list_of_list_ranks_system, K=60):\n",
    "    \"\"\"\n",
    "    Объединение результатов ранжирования от нескольких IR систем с помощью Reciprocal Rank Fusion.\n",
    "\n",
    "    Args:\n",
    "    list_of_list_ranks_system: Ранжированные результаты от разных IR систем.\n",
    "    K (int): Константа, используемая в формуле RRF (по умолчанию 60, просто часто используют 60).\n",
    "\n",
    "    Returns:\n",
    "    Кортеж из списка отсортированных документов по оценке и отсортированных документов\n",
    "    \"\"\"\n",
    "    # Словарь для хранения RRF оценок\n",
    "    rrf_map = defaultdict(float)\n",
    "\n",
    "    # Вычисляем RRF оценку для каждого результата в каждом списке\n",
    "    for rank_list in list_of_list_ranks_system:\n",
    "        for rank, item in enumerate(rank_list, 1):\n",
    "            rrf_map[item] += 1 / (rank + K)\n",
    "\n",
    "    # Сортируем элементы по их RRF оценкам в порядке убывания\n",
    "    sorted_items = sorted(rrf_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Возвращаем кортеж из списка отсортированных документов по оценке и отсортированных документов\n",
    "    return sorted_items, [item for item, score in sorted_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_top_k = vector_retrieval(query, top_k=5)\n",
    "bm25_top_k = bm25_retrieval(query, top_k=5)\n",
    "\n",
    "hybrid_top_k = reciprocal_rank_fusion(vector_top_k, bm25_top_k)\n",
    "hybrid_top_k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in hybrid_top_k[1]:\n",
    "  print(f\"Chunk Index {index} : {filtered_chunks[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы улучшим качество результатов, применяя реранкинг с учетом окружающего контекста. Это позволит нам более точно оценить релевантность документов, основываясь на их содержании и связи с запросом. Реранкинг поможет выделить наиболее подходящие документы, учитывая не только их первоначальные оценки, но и контекст, в котором они были найдены. Воспользуемся легким с hf и самым используемым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [filtered_chunks[idx] for idx in hybrid_top_k[1]]\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "pairs = [[query, doc] for doc in documents]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "reranked_results = list(zip(documents, scores))\n",
    "reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Top-3 results after reranking:\\n\")\n",
    "for doc, score in reranked_results[:3]:\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Relevance score: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_chunks = ''\n",
    "\n",
    "for doc, score in reranked_results:\n",
    "    retreived_chunks += doc + '\\n\\n'\n",
    "\n",
    "print(retreived_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Answer the question: {query}. Here is relevant information: {retreived_chunks} Response only in Russian. It is necessary to implement everything in detail in the code. You are a coding assistant.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompt_templates import bm25_rag_prompt\n",
    "\n",
    "class BM25RAG:\n",
    "    def __init__(self, embeddings: List[np.ndarray], chunks: List[str], client: OpenAI, prompt_template: str = bm25_rag_prompt):\n",
    "        \"\"\"\n",
    "        Инициализация класса BM25RAG.\n",
    "\n",
    "        Args:\n",
    "            embeddings (List[np.ndarray]): Эмбеддинги документов\n",
    "            chunks (List[str]): Исходные текстовые чанки\n",
    "            client (OpenAI): клиент OpenAI\n",
    "            prompt_template (str): Шаблон промпта\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = chunks\n",
    "        self.client = client\n",
    "        self.prompt_template = prompt_template\n",
    "        self.bm25_retriever = bm25s.BM25(corpus=chunks)\n",
    "        self.bm25_retriever.index(bm25s.tokenize(chunks))\n",
    "        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        \n",
    "    def _vector_retrieval(self, query: str, top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Получение top-k наиболее релевантных документов на основе векторного поиска.\n",
    "        \"\"\"\n",
    "        query_embedding = generate_embeddings([query])[0]\n",
    "        embeddings_array = np.array(self.embeddings)\n",
    "        similarities = cosine_similarity([query_embedding], embeddings_array)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return list(top_indices)\n",
    "    \n",
    "    def _bm25_retrieval(self, query: str, top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Получение top-k наиболее релевантных документов с помощью BM25.\n",
    "        \"\"\"\n",
    "        results, _ = self.bm25_retriever.retrieve(bm25s.tokenize(query), k=top_k)\n",
    "        return [self.chunks.index(doc) for doc in results[0]]\n",
    "    \n",
    "    def _rerank_results(self, query: str, documents: List[str], top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Реранкинг результатов с использованием кросс-энкодера.\n",
    "        \"\"\"\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        reranked_results = list(zip(documents, scores))\n",
    "        reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in reranked_results[:top_k]]\n",
    "\n",
    "    def retrieve_and_generate(self, query: str, top_k: int = 5) -> Tuple[List[str], str]:\n",
    "        \"\"\"\n",
    "        Получение релевантных чанков и генерация ответа.\n",
    "\n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "            top_k (int): Количество релевантных чанков\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], str]: Кортеж из списка использованных чанков и сгенерированного ответа\n",
    "        \"\"\"\n",
    "        # Получаем результаты от обоих методов\n",
    "        vector_top_k = self._vector_retrieval(query, top_k)\n",
    "        bm25_top_k = self._bm25_retrieval(query, top_k)\n",
    "        \n",
    "        # Объединяем результаты с помощью RRF\n",
    "        _, hybrid_indices = reciprocal_rank_fusion(vector_top_k, bm25_top_k)\n",
    "        \n",
    "        # Получаем документы для реранкинга\n",
    "        documents = [self.chunks[idx] for idx in hybrid_indices]\n",
    "        \n",
    "        # Выполняем реранкинг\n",
    "        reranked_chunks = self._rerank_results(query, documents)\n",
    "        \n",
    "        # Объединяем контекст\n",
    "        context = \"\\n\\n\".join(reranked_chunks)\n",
    "        \n",
    "        # Генерируем ответ\n",
    "        prompt = self.prompt_template.format(\n",
    "            context=context,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return reranked_chunks, response.choices[0].message.content\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> Tuple[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Основной метод для получения ответа на запрос.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "            top_k (int): Количество релевантных чанков\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[str, List[int]]: Кортеж из сгенерированного ответа и индексов использованных чанков\n",
    "        \"\"\"\n",
    "        relevant_chunks, answer = self.retrieve_and_generate(query, top_k)\n",
    "        chunk_indices = [self.chunks.index(chunk) for chunk in relevant_chunks]\n",
    "        return answer, chunk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_rag = BM25RAG(embeddings=embeddings, chunks=chunks, client=client)  \n",
    "bm25_response, _ = bm25_rag.retrieve(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bm25_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Построение Knowledge graph</font>\n",
    "\n",
    "На основе текущих возможностей фремворка langchain построим наш knowledge граф знаний на основе текущего репозитория."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "class KnowledgeBaseORM:\n",
    "    def __init__(self):\n",
    "        self.graph = Neo4jGraph(\n",
    "            url=os.getenv(\"NEO4J_URL\"),\n",
    "            username=os.getenv(\"NEO4J_USERNAME\", \"neo4j\"),\n",
    "            password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "        )\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "        self.llm_transformer = LLMGraphTransformer(\n",
    "            llm=self.llm,\n",
    "            allowed_nodes=[\n",
    "                \"Class\", \"Method\", \"Function\", \"Interface\",\n",
    "                \"Controller\", \"Service\", \"Repository\", \"Entity\", \"DTO\",\n",
    "                \"Variable\", \"Parameter\", \"ReturnType\"\n",
    "            ],\n",
    "            allowed_relationships=[\n",
    "                (\"Class\", \"IMPLEMENTS\", \"Interface\"),\n",
    "                (\"Class\", \"EXTENDS\", \"Class\"),\n",
    "                (\"Method\", \"BELONGS_TO\", \"Class\"),\n",
    "                (\"Method\", \"RETURNS\", \"ReturnType\"),\n",
    "                (\"Method\", \"ACCEPTS\", \"Parameter\"),\n",
    "                (\"Controller\", \"USES\", \"Service\"),\n",
    "                (\"Service\", \"USES\", \"Repository\"),\n",
    "                (\"Repository\", \"MANAGES\", \"Entity\"),\n",
    "                (\"Controller\", \"RETURNS\", \"DTO\"),\n",
    "                (\"Entity\", \"MAPS_TO\", \"DTO\"),\n",
    "                (\"Service\", \"TRANSFORMS\", \"DTO\"),\n",
    "                (\"Function\", \"CALLS\", \"Function\"),\n",
    "                (\"Method\", \"CALLS\", \"Method\"),\n",
    "                (\"Variable\", \"TYPE_OF\", \"Class\")\n",
    "            ],\n",
    "            node_properties=True,\n",
    "        )\n",
    "        self.total_cost = 0\n",
    "        self.total_tokens = 0\n",
    "        self.timeout = 180\n",
    "\n",
    "    def load_documents(self, chunks: list):\n",
    "        \"\"\"\n",
    "        Load documents from a list of chunks and convert them into graph documents.\n",
    "        Analyzes the code for:\n",
    "        - Classes and their hierarchy\n",
    "        - Methods and their signatures\n",
    "        - Used data types\n",
    "        - Relationships between components\n",
    "        - Architectural patterns\n",
    "\n",
    "        Args:\n",
    "            chunks (list): List of text chunks to load.\n",
    "        \"\"\"\n",
    "        for idx, chunk in enumerate(chunks, 1):\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                with get_openai_callback() as cb:\n",
    "                    if isinstance(chunk, tuple):\n",
    "                        document = Document(page_content=chunk[0])\n",
    "                    else:\n",
    "                        document = Document(page_content=chunk)\n",
    "                    \n",
    "                    if time.time() - start_time > 180:  # Increased timeout to 3 minutes\n",
    "                        print(f\"Chunk {idx} skipped due to exceeding processing time (>180 sec)\")\n",
    "                        continue\n",
    "                        \n",
    "                    graph_documents = self.llm_transformer.convert_to_graph_documents([document])\n",
    "                    self.graph.add_graph_documents(graph_documents)\n",
    "                    \n",
    "                    print(f\"Chunk {idx} successfully analyzed and added to the knowledge graph.\")\n",
    "                    print(f\"Cost of analyzing chunk {idx}: ${cb.total_cost:.4f}\")\n",
    "                    print(f\"Tokens used: {cb.total_tokens}\")\n",
    "                    print(f\"Analysis time: {time.time() - start_time:.2f} sec\\n\")\n",
    "                    \n",
    "                    self.total_cost += cb.total_cost\n",
    "                    self.total_tokens += cb.total_tokens\n",
    "                    \n",
    "                    # Adding a 5-second delay between requests\n",
    "                    time.sleep(5)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing chunk {idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def clear_graph(self):\n",
    "        \"\"\"\n",
    "        Clear the existing knowledge graph and prepare for new analysis.\n",
    "        \"\"\"\n",
    "        self.graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"Knowledge graph cleared for new analysis.\")\n",
    "\n",
    "    def build_knowledge_graph(self, filtered_chunks: list):\n",
    "        \"\"\"\n",
    "        Build a knowledge graph from the codebase.\n",
    "        Analyzes the structure of the code and creates relationships between components.\n",
    "\n",
    "        Args:\n",
    "            filtered_chunks (list): List of filtered code chunks.\n",
    "        \"\"\"\n",
    "        self.clear_graph()\n",
    "        self.load_documents(filtered_chunks)\n",
    "        print(\"\\nFinal analysis statistics:\")\n",
    "        print(f\"Total analysis cost: ${self.total_cost:.4f}\")\n",
    "        print(f\"Total number of processed tokens: {self.total_tokens}\")\n",
    "        print(\"Knowledge graph of the codebase successfully built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orm = KnowledgeBaseORM()\n",
    "# orm.build_knowledge_graph(filtered_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from src.prompt_templates import graph_rag_prompt\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, embeddings: List[np.ndarray], chunks: List[str], client: OpenAI, prompt_template: str = graph_rag_prompt):\n",
    "        \"\"\"\n",
    "        Инициализация класса GraphRAG.\n",
    "\n",
    "        Args:\n",
    "            embeddings (List[np.ndarray]): Эмбеддинги документов\n",
    "            chunks (List[str]): Исходные текстовые чанки\n",
    "            client (OpenAI): клиент OpenAI\n",
    "            prompt_template (str): Шаблон промпта\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = chunks\n",
    "        self.client = client\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "        # Создаем LLM из langchain\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=0.2,\n",
    "            model_name=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    "        )\n",
    "        \n",
    "        # Подключение к Neo4j\n",
    "        self.graph = Neo4jGraph(\n",
    "            url=os.getenv(\"NEO4J_URL\"),\n",
    "            username=os.getenv(\"NEO4J_USERNAME\", \"neo4j\"),\n",
    "            password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "            database=os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "        )\n",
    "        \n",
    "        # Инициализация цепочек запросов с langchain LLM\n",
    "        self.class_chain = GraphCypherQAChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            graph=self.graph,\n",
    "            verbose=True,\n",
    "            allow_dangerous_requests=True\n",
    "        )\n",
    "        \n",
    "        self.method_chain = GraphCypherQAChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            graph=self.graph,\n",
    "            verbose=True,\n",
    "            allow_dangerous_requests=True\n",
    "        )\n",
    "        \n",
    "        # Промпты для объединения контекстов и генерации кода\n",
    "        self.combine_prompt = PromptTemplate(\n",
    "            input_variables=[\"class_context\", \"method_context\", \"user_question\"],\n",
    "            template=(\n",
    "                \"Контекст по классам и интерфейсам:\\n{class_context}\\n\\n\"\n",
    "                \"Контекст по методам:\\n{method_context}\\n\\n\"\n",
    "                \"Пользовательский запрос:\\n{user_question}\\n\\n\"\n",
    "                \"Сформируй итоговое описание, которое объединяет оба результата.\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.code_prompt = PromptTemplate(\n",
    "            input_variables=[\"combined_context\", \"user_question\"],\n",
    "            template=(\n",
    "                \"Ниже приведён агрегированный контекст, полученный из базы знаний Neo4j:\\n\"\n",
    "                \"{combined_context}\\n\\n\"\n",
    "                \"Пользовательский запрос:\\n\"\n",
    "                \"{user_question}\\n\\n\"\n",
    "                \"На основе приведённого контекста сгенерируй код на Spring Boot для реализации API.\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.combine_chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.combine_prompt,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        self.code_chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.code_prompt,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def _vector_retrieval(self, query: str, top_k: int = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Получение top-k наиболее релевантных документов на основе векторного поиска.\n",
    "        \"\"\"\n",
    "        query_embedding = generate_embeddings([query])[0]\n",
    "        embeddings_array = np.array(self.embeddings)\n",
    "        similarities = cosine_similarity([query_embedding], embeddings_array)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return list(top_indices)\n",
    "\n",
    "    def retrieve_and_generate(self, query: str) -> Tuple[List[str], str]:\n",
    "        \"\"\"\n",
    "        Получение контекста из графа знаний и генерация ответа.\n",
    "\n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], str]: Кортеж из списка использованных контекстов и сгенерированного ответа\n",
    "        \"\"\"\n",
    "        for attempt in range(10):\n",
    "            try:\n",
    "                # Получаем контекст по классам и интерфейсам\n",
    "                class_context = self.class_chain.run(\n",
    "                    \"Какие классы и интерфейсы задействованы? \" + query\n",
    "                )\n",
    "\n",
    "                # Получаем контекст по методам\n",
    "                method_context = self.method_chain.run(\n",
    "                    \"Какие методы используются в данных классах? \" + query\n",
    "                )\n",
    "\n",
    "                # Получаем результаты векторного поиска\n",
    "                vector_top_k = self._vector_retrieval(query)\n",
    "                vector_context = [self.chunks[idx] for idx in vector_top_k]\n",
    "\n",
    "                # Объединяем контексты\n",
    "                combined_context = self.combine_chain.run(\n",
    "                    class_context=class_context,\n",
    "                    method_context=method_context,\n",
    "                    user_question=query\n",
    "                )\n",
    "\n",
    "                # Добавляем контекст из векторного поиска\n",
    "                combined_context += \"\\n\\n\" + \"\\n\".join(vector_context)\n",
    "\n",
    "                # Генерируем ответ\n",
    "                response = self.code_chain.run(\n",
    "                    combined_context=combined_context,\n",
    "                    user_question=query\n",
    "                )\n",
    "\n",
    "                contexts = [class_context, method_context, combined_context]\n",
    "                return contexts, response\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt == 9:  # Если это последняя попытка\n",
    "                    return [], \"\"  # Возвращаем пустой контекст\n",
    "                continue  # Пытаемся снова\n",
    "\n",
    "    def retrieve(self, query: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Основной метод для получения ответа на запрос.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Запрос пользователя\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[str, List[str]]: Кортеж из сгенерированного ответа и использованных контекстов\n",
    "        \"\"\"\n",
    "        contexts, answer = self.retrieve_and_generate(query)\n",
    "        return answer, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = GraphRAG(embeddings=embeddings, chunks=filtered_chunks, client=client)\n",
    "response, _ = graph_rag.retrieve(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Ragas</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "def generate_test_cases(chunks: List[str], n_samples: int = 30) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Генерация тестовых примеров с помощью LLM\n",
    "    \"\"\"\n",
    "    test_cases = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Случайно выбираем чанк\n",
    "        chunk = random.choice(chunks)\n",
    "        \n",
    "        # Генерируем вопрос на основе чанка\n",
    "        prompt = f\"\"\"На основе следующего фрагмента кода сгенерируй технический вопрос:\n",
    "        {chunk}\n",
    "        \n",
    "        Вопрос должен быть связан с реализацией конкретной функциональности.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        question = response.choices[0].message.content\n",
    "        \n",
    "        # Генерируем эталонный ответ на основе контекста\n",
    "        reference_prompt = f\"\"\"На основе следующего фрагмента кода дай подробный ответ на вопрос:\n",
    "        \n",
    "        Контекст: {chunk}\n",
    "        \n",
    "        Вопрос: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        reference_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=[{\"role\": \"user\", \"content\": reference_prompt}]\n",
    "        )\n",
    "        \n",
    "        reference_answer = reference_response.choices[0].message.content\n",
    "        \n",
    "        test_cases.append({\n",
    "            \"question\": question,\n",
    "            \"context\": chunk,\n",
    "            \"reference_answer\": reference_answer\n",
    "        })\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "def evaluate_rag_system(test_cases: List[dict], rag_system) -> dict:\n",
    "    \"\"\"\n",
    "    Формирование датасета RAG системы \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # Добавляем обработку ошибок с 5 попытками\n",
    "        max_attempts = 5\n",
    "        attempt = 0\n",
    "        answer = None\n",
    "        \n",
    "        while attempt < max_attempts:\n",
    "            try:\n",
    "                answer = rag_system.retrieve(case[\"question\"])\n",
    "                break\n",
    "            except Exception:\n",
    "                attempt += 1\n",
    "                if attempt == max_attempts:\n",
    "                    print(f\"Пропускаем вопрос после {max_attempts} неудачных попыток: {case['question']}\")\n",
    "                    continue\n",
    "        \n",
    "        if answer is not None:\n",
    "            results.append({\n",
    "                \"question\": case[\"question\"],\n",
    "                \"context\": case[\"context\"],\n",
    "                \"answer\": answer,\n",
    "                \"reference_answer\": case[\"reference_answer\"]\n",
    "            })\n",
    "    \n",
    "    return {\"samples\": results}\n",
    "\n",
    "# # Генерируем тестовые примеры\n",
    "# test_cases = generate_test_cases(filtered_chunks)\n",
    "\n",
    "# #Оцениваем Simple RAG\n",
    "# simple_rag_dataset = evaluate_rag_system(test_cases, SimpleRAG(embeddings=embeddings, chunks=filtered_chunks, client=client))\n",
    "\n",
    "# #Оцениваем Graph RAG\n",
    "# graph_rag_dataset = evaluate_rag_system(test_cases, GraphRAG(embeddings=embeddings, chunks=filtered_chunks, client=client))\n",
    "\n",
    "# #Оцениваем RAG + BM25\n",
    "# bm25_rag_dataset = evaluate_rag_system(test_cases, BM25RAG(embeddings=embeddings, chunks=filtered_chunks, client=client))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSimple RAG results:\")\n",
    "pprint(simple_rag_dataset)\n",
    "print(\"Graph RAG results:\")\n",
    "pprint(graph_rag_dataset)\n",
    "print(\"\\nRAG + BM25 results:\")\n",
    "pprint(bm25_rag_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "def prepare_dataset(\n",
    "    test_cases: List[dict],  \n",
    "    system_answers: List[str]\n",
    ") -> EvaluationDataset:\n",
    "    \"\"\"\n",
    "    Подготовка датасета для ragas.\n",
    "    test_cases: список словарей с ключами 'question', 'context', 'reference_answer'\n",
    "    system_answers: список ответов от RAG (по одному ответу на каждый test_case)\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for case, answer in zip(test_cases, system_answers):\n",
    "        # Убедимся, что ответ является строкой\n",
    "        if not isinstance(answer, str):\n",
    "            answer = str(answer)\n",
    "\n",
    "        # reference – это эталонный ответ\n",
    "        # actual_output (или output) – это фактический ответ системы\n",
    "        # question/context – входная пара\n",
    "        samples.append(\n",
    "            SingleTurnSample(\n",
    "                question=case[\"question\"],\n",
    "                context=case[\"context\"],\n",
    "                actual_output=answer,             # фактический ответ\n",
    "                user_input=case[\"question\"],\n",
    "                response=answer,                 # дублируем для удобства\n",
    "                retrieved_contexts=[case[\"context\"]],\n",
    "                reference=case[\"reference_answer\"]  # эталонный ответ\n",
    "            )\n",
    "        )\n",
    "    return EvaluationDataset(samples=samples)\n",
    "\n",
    "# Извлекаем ответы из датасетов систем\n",
    "graph_rag_answers  = [sample[\"answer\"] for sample in graph_rag_dataset[\"samples\"]]\n",
    "bm25_rag_answers   = [sample[\"answer\"] for sample in bm25_rag_dataset[\"samples\"]]\n",
    "simple_rag_answers = [sample[\"answer\"] for sample in simple_rag_dataset[\"samples\"]]\n",
    "\n",
    "# Формируем EvaluationDataset для каждой системы\n",
    "simple_rag_data = prepare_dataset(test_cases, simple_rag_answers)\n",
    "graph_rag_data  = prepare_dataset(test_cases, graph_rag_answers)\n",
    "bm25_rag_data   = prepare_dataset(test_cases, bm25_rag_answers)\n",
    "\n",
    "# Запускаем оценку по метрикам\n",
    "metrics = [faithfulness, answer_relevancy, context_recall]\n",
    "\n",
    "simple_rag_results = evaluate(simple_rag_data, metrics)\n",
    "graph_rag_results  = evaluate(graph_rag_data, metrics)\n",
    "bm25_rag_results   = evaluate(bm25_rag_data, metrics)\n",
    "\n",
    "print(\"\\nSimple RAG Results:\")\n",
    "for metric in metrics:\n",
    "    metric_name = metric.name\n",
    "    print(f\"{metric_name}: {simple_rag_results[metric_name][0]:.2f}\")\n",
    "\n",
    "print(\"\\nGraph RAG Results:\")\n",
    "for metric in metrics:\n",
    "    metric_name = metric.name\n",
    "    print(f\"{metric_name}: {graph_rag_results[metric_name][0]:.2f}\")\n",
    "\n",
    "print(\"\\nBM25 RAG Results:\")\n",
    "for metric in metrics:\n",
    "    metric_name = metric.name\n",
    "    print(f\"{metric_name}: {bm25_rag_results[metric_name][0]:.2f}\")\n",
    "\n",
    "\n",
    "# Сравнение результатов – определяем, у какой из систем максимальный скор\n",
    "print(\"\\nComparison:\")\n",
    "for metric in metrics:\n",
    "    metric_name   = metric.name\n",
    "    simple_score  = simple_rag_results[metric_name][0]\n",
    "    graph_score   = graph_rag_results[metric_name][0]\n",
    "    bm25_score    = bm25_rag_results[metric_name][0]\n",
    "\n",
    "    scores = {\n",
    "        \"Simple RAG\": simple_score,\n",
    "        \"Graph RAG\":  graph_score,\n",
    "        \"BM25 RAG\":   bm25_score\n",
    "    }\n",
    "    best_system = max(scores, key=scores.get)\n",
    "\n",
    "    print(\n",
    "        f\"{metric_name}: {best_system} performs better | \"\n",
    "        f\"Simple RAG: {simple_score:.2f}, \"\n",
    "        f\"Graph RAG: {graph_score:.2f}, \"\n",
    "        f\"BM25 RAG: {bm25_score:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
